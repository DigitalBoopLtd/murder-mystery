"""Parallel mystery generation using sub-agents.

This module replaces the monolithic mystery generation with parallel sub-agents:
- 4x faster startup (~6s vs ~15s)
- Better error isolation (one suspect failing doesn't lose everything)
- More consistent character development (each suspect gets full LLM attention)

Architecture:
    1. Skeleton Agent → Premise + murderer + role outlines (~2s)
    2. Parallel Agents → 4 suspects + clues (all run simultaneously ~5s)
    3. Assembly → Combine outputs + assign voices

IMPORTANT: This is NOT the agent the user talks to. The user talks to the
Game Master Agent in services/agent.py. This module only runs once at
game start to generate the mystery content.
"""

import asyncio
import logging
import os
import random
from typing import List, Optional, Tuple

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field

from game.models import (
    Mystery, Suspect, Victim, Clue, MysteryPremise,
    AlibiClaim, WitnessStatement, MurderMethod
)
from mystery_config import MysteryConfig

logger = logging.getLogger(__name__)

# Retry configuration
MAX_RETRIES = 3
RETRY_DELAY_SECONDS = 1.0


# =============================================================================
# INTERMEDIATE MODELS FOR SUB-AGENTS
# =============================================================================

class MysterySkeleton(BaseModel):
    """Lightweight skeleton that guides parallel generation.
    
    This is generated first to establish the framework that all
    parallel sub-agents will work within.
    """
    setting: str = Field(description="Vivid 1-2 sentence setting description")
    victim_name: str = Field(description="Victim's full name")
    victim_background: str = Field(description="1-2 sentences about the victim")
    murderer_index: int = Field(
        ge=0, le=3, 
        description="Which suspect (0-3) is the murderer"
    )
    weapon: str = Field(description="Murder weapon")
    motive: str = Field(description="Why the murderer did it")
    suspect_briefs: List[str] = Field(
        min_length=4, max_length=4,
        description="4 brief role descriptions like 'The jealous business partner'"
    )
    clue_locations: List[str] = Field(
        min_length=5, max_length=5,
        description="5 specific locations where clues will be found"
    )
    # Murder timeline for alibi verification
    murder_time: str = Field(
        default="9:00 PM",
        description="Approximate time of murder (e.g., '9:00 PM')"
    )
    murder_location: str = Field(
        default="",
        description="Where the murder took place"
    )


class AlibiDraft(BaseModel):
    """Structured alibi for verification system."""
    time_claimed: str = Field(description="Time range: e.g., '8:30 PM - 10:00 PM'")
    location_claimed: str = Field(description="Where they claim to have been")
    activity: str = Field(description="What they were doing")
    corroborator: Optional[str] = Field(
        default=None,
        description="Role of another suspect who can verify this (e.g., 'the business partner')"
    )
    corroboration_type: str = Field(
        default="none",
        description="'witness' (another suspect saw them), 'physical' (clue proves it), or 'none' (alone)"
    )
    is_truthful: bool = Field(default=True, description="False only for the murderer's fake alibi")


class SuspectDraft(BaseModel):
    """Output from a single suspect sub-agent."""
    name: str = Field(description="Full character name")
    role: str = Field(description="Relationship to victim")
    personality: str = Field(description="2-3 key personality traits")
    alibi: str = Field(description="Simple alibi statement for display")
    secret: str = Field(description="What they're hiding")
    clue_they_know: str = Field(description="Info they might share if pressed")
    gender: str = Field(description="male or female")
    age: str = Field(description="young, middle_aged, or old")
    nationality: str = Field(description="american, british, australian, or standard")
    # Structured alibi for verification
    structured_alibi: AlibiDraft = Field(description="Detailed alibi with verification method")
    # What they saw (witness statement about another suspect)
    witness_claim: Optional[str] = Field(
        default=None,
        description="What they saw another suspect do, e.g., 'I saw the professor in the library at 8:45'"
    )
    witness_subject_role: Optional[str] = Field(
        default=None,
        description="Role of the suspect they're making a statement about"
    )
    # location_hint is assigned during assembly, not generated by the sub-agent


class ClueDraft(BaseModel):
    """Output for a single clue with alibi verification capabilities."""
    id: str = Field(description="Unique clue ID like clue_1")
    description: str = Field(description="What the clue is")
    location: str = Field(description="Where it's found")
    significance: str = Field(description="What it means for the case")
    # Alibi verification
    contradicts_alibi_of_role: Optional[str] = Field(
        default=None,
        description="Role of suspect whose alibi this DISPROVES (e.g., 'the jealous partner')"
    )
    supports_alibi_of_role: Optional[str] = Field(
        default=None,
        description="Role of suspect whose alibi this CONFIRMS"
    )
    timeline_implication: Optional[str] = Field(
        default=None,
        description="What this tells us about timing"
    )
    evidence_type: str = Field(
        default="circumstantial",
        description="'physical', 'documentary', or 'circumstantial'"
    )


class ClueSet(BaseModel):
    """Output from the clue sub-agent."""
    clues: List[ClueDraft] = Field(min_length=5, max_length=5)


# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

async def retry_with_backoff(coro_func, *args, max_retries=MAX_RETRIES, **kwargs):
    """Retry an async function with exponential backoff.
    
    Args:
        coro_func: Async function to call
        *args: Arguments to pass to the function
        max_retries: Maximum number of retry attempts
        **kwargs: Keyword arguments to pass to the function
        
    Returns:
        Result of the successful call
        
    Raises:
        Last exception if all retries fail
    """
    last_error = None
    for attempt in range(max_retries):
        try:
            return await coro_func(*args, **kwargs)
        except Exception as e:
            last_error = e
            if attempt < max_retries - 1:
                delay = RETRY_DELAY_SECONDS * (2 ** attempt)  # Exponential backoff
                logger.warning(
                    "[PARALLEL] Attempt %d/%d failed: %s. Retrying in %.1fs...",
                    attempt + 1, max_retries, str(e)[:100], delay
                )
                await asyncio.sleep(delay)
            else:
                logger.error(
                    "[PARALLEL] All %d attempts failed. Last error: %s",
                    max_retries, str(e)[:200]
                )
    raise last_error


# =============================================================================
# SUB-AGENT: SKELETON GENERATOR
# =============================================================================

async def _generate_skeleton_impl(
    config: Optional[MysteryConfig] = None,
    premise: Optional[MysteryPremise] = None,
) -> MysterySkeleton:
    """Internal implementation of skeleton generation."""
    from game.mystery_generator import SETTING_TYPES
    
    # Use structured output - LLM is constrained to output valid Pydantic
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.9,
        api_key=os.getenv("OPENAI_API_KEY"),
    ).with_structured_output(MysterySkeleton)
    
    # Use premise if provided, otherwise generate setting
    if premise:
        setting_instruction = f"""Use this EXACT setting and victim:
Setting: {premise.setting}
Victim: {premise.victim_name} - {premise.victim_background}

Do NOT change the setting or victim details."""
    else:
        setting_type = random.choice(SETTING_TYPES) if not config else config.get_setting_for_generation()
        setting_instruction = f"Create a vivid setting based on: {setting_type}"
    
    tone_instruction = ""
    if config:
        tone = config.get_tone_instruction()
        if tone:
            tone_instruction = f"\nTONE: {tone}"
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are planning a murder mystery game structure.

Your job is to create a SKELETON - the framework that guides detailed generation.
This is NOT the final mystery, just the blueprint.

REQUIREMENTS:
1. Pick which of 4 suspects (index 0-3) will be the murderer
2. Create 4 distinct suspect ROLE BRIEFS (not full characters yet)
3. Choose weapon and motive
4. List 5 specific searchable locations for clues

SUSPECT ROLE BRIEFS should be evocative like:
- "The bitter ex-business partner who lost everything"
- "The charming assistant with a dark secret"
- "The victim's estranged child seeking inheritance"
- "The rival who was publicly humiliated"

{tone_instruction}"""),
        ("human", """{setting_instruction}

Generate the mystery skeleton with:
- Setting and victim details
- 4 suspect role briefs (one will be the murderer)
- murderer_index (0-3) indicating which suspect is guilty
- Weapon and motive
- 5 specific clue locations fitting the setting""")
    ])
    
    chain = prompt | llm
    
    # Structured output returns the Pydantic model directly - no parsing needed!
    skeleton = await chain.ainvoke({
        "setting_instruction": setting_instruction,
        "tone_instruction": tone_instruction,
    })
    
    logger.info(
        "[PARALLEL] Skeleton: murderer_index=%d, weapon=%s, %d locations",
        skeleton.murderer_index,
        skeleton.weapon,
        len(skeleton.clue_locations),
    )
    return skeleton


async def generate_skeleton(
    config: Optional[MysteryConfig] = None,
    premise: Optional[MysteryPremise] = None,
) -> MysterySkeleton:
    """Stage 1: Generate the mystery skeleton (fast, ~2s).
    
    This determines the structure that all parallel agents will follow.
    Uses gpt-4o-mini for speed since this is just the framework.
    Uses structured output for reliable parsing.
    """
    logger.info("[PARALLEL] Generating skeleton...")
    return await retry_with_backoff(_generate_skeleton_impl, config, premise)


# =============================================================================
# SUB-AGENT: SUSPECT GENERATOR (runs 4x in parallel)
# =============================================================================

async def _generate_suspect_impl(
    skeleton: MysterySkeleton,
    role_brief: str,
    suspect_index: int,
    is_guilty: bool,
    voice_options: Optional[str] = None,
) -> SuspectDraft:
    """Internal implementation of suspect generation with structured output."""
    # Use structured output - LLM is constrained to output valid Pydantic
    llm = ChatOpenAI(
        model="gpt-4o",
        temperature=0.9,
        api_key=os.getenv("OPENAI_API_KEY"),
    ).with_structured_output(SuspectDraft)
    
    # Get other suspect roles for cross-referencing (for witness statements)
    other_roles = [r for i, r in enumerate(skeleton.suspect_briefs) if i != suspect_index]
    other_roles_str = ", ".join(other_roles) if other_roles else "other suspects"
    
    if is_guilty:
        guilt_instructions = f"""
⚠️ THIS SUSPECT IS THE MURDERER ⚠️

They killed the victim using: {skeleton.weapon}
Their motive: {skeleton.motive}
Murder time: Around {skeleton.murder_time}
Murder location: {skeleton.murder_location or skeleton.clue_locations[0]}

CHARACTER RULES FOR THE GUILTY:
- Their alibi is FALSE - they claim to be somewhere they weren't
- structured_alibi.is_truthful MUST be False
- Their alibi should sound plausible but can be DISPROVEN by:
  1. A clue at a location (physical evidence they were elsewhere)
  2. Another suspect's testimony contradicting their claim
- If they claim someone can corroborate, that person should NOT actually confirm it
- Their secret relates to their guilt but doesn't directly confess
- Their "clue_they_know" should be misleading or deflecting

ALIBI CREATION (GUILTY):
- Create a specific alibi with exact time/place that COVERS the murder time
- The alibi should be falsifiable - there should be a way to prove they weren't there
- corroboration_type should be 'witness' if they falsely claim someone saw them
- corroborator should be one of the other roles: {other_roles_str}
"""
    else:
        guilt_instructions = f"""
This suspect is INNOCENT but should still seem suspicious.

CHARACTER RULES FOR THE INNOCENT:
- Give them their OWN secret unrelated to the murder
- Their alibi is TRUE - they were where they say they were
- structured_alibi.is_truthful MUST be True
- Their alibi can be verified by another suspect OR was alone
- They may have HAD motive but didn't act on it
- Their "clue_they_know" should be helpful info they might share

ALIBI CREATION (INNOCENT):
- Create a REAL alibi with specific time/place
- corroboration_type options:
  - 'witness': Another suspect saw them (corroborator = one of: {other_roles_str})
  - 'physical': A clue at a location proves their presence
  - 'none': They were alone (harder to verify but also can't disprove)
- If they have a witness corroborator, it should be truthful
"""
    
    voice_block = ""
    if voice_options:
        voice_block = f"""
VOICE CASTING:
Design this character to match one of these available voice actors.
Consider gender, age range, and accent when creating the character:

{voice_options[:1500]}

Pick characteristics that fit an available voice well."""
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are creating ONE detailed suspect for a murder mystery.

You are building a THREE-DIMENSIONAL character, not a cardboard cutout.
Give them depth, quirks, and believable motivations.

SETTING: {setting}
VICTIM: {victim_name} - {victim_background}
MURDER TIME: Around {murder_time}
OTHER SUSPECTS IN THIS CASE: {other_roles}

{guilt_instructions}

{voice_block}

QUALITY REQUIREMENTS:
- Name should fit the setting and feel authentic (no quotes or nicknames in the name)
- Personality should be specific, not generic ("nervous and detail-oriented" not just "suspicious")
- Secret should be juicy and character-defining
- Their "clue_they_know" should be something they'd realistically know

ALIBI REQUIREMENTS (CRITICAL):
- The "alibi" field is a simple statement like "I was in the library reading"
- The "structured_alibi" object contains detailed verification info:
  - time_claimed: Specific time range covering the murder time
  - location_claimed: Specific place
  - activity: What they were doing
  - corroborator: Role of witness (if any) from: {other_roles}
  - corroboration_type: 'witness', 'physical', or 'none'
  - is_truthful: True for innocent suspects, False for the murderer

WITNESS STATEMENT (optional):
- If this character saw another suspect during the relevant time, include:
  - witness_claim: "I saw [role] at [location] around [time]"
  - witness_subject_role: The role of the person they saw
- This helps corroborate (or contradict) other alibis"""),
        ("human", """Create a fully realized character for this role:
"{role_brief}"

Make them memorable and distinct from typical mystery tropes.
Include a detailed structured_alibi that can be verified or disproven.""")
    ])
    
    chain = prompt | llm
    
    # Structured output returns the Pydantic model directly - no parsing needed!
    suspect = await chain.ainvoke({
        "setting": skeleton.setting,
        "victim_name": skeleton.victim_name,
        "victim_background": skeleton.victim_background,
        "murder_time": skeleton.murder_time,
        "other_roles": other_roles_str,
        "guilt_instructions": guilt_instructions,
        "voice_block": voice_block,
        "role_brief": role_brief,
    })
    
    return suspect


async def generate_suspect(
    skeleton: MysterySkeleton,
    role_brief: str,
    suspect_index: int,
    is_guilty: bool,
    voice_options: Optional[str] = None,
) -> Tuple[SuspectDraft, int, bool]:
    """Generate a single suspect (runs in parallel with other suspects).
    
    Uses gpt-4o for quality character development since each suspect
    gets full LLM attention in parallel. Uses structured output for
    reliable parsing with automatic retries.
    
    Returns:
        Tuple of (suspect_draft, suspect_index, is_guilty)
    """
    logger.info("[PARALLEL] Generating suspect %d: %s (guilty=%s)", 
                suspect_index, role_brief[:40], is_guilty)
    
    suspect = await retry_with_backoff(
        _generate_suspect_impl, skeleton, role_brief, suspect_index, is_guilty, voice_options
    )
    
    logger.info("[PARALLEL] ✓ Suspect %d: %s (%s)", suspect_index, suspect.name, suspect.role)
    return (suspect, suspect_index, is_guilty)


# =============================================================================
# SUB-AGENT: CLUE GENERATOR
# =============================================================================

async def _generate_clues_impl(
    skeleton: MysterySkeleton,
    murderer_role: str,
) -> ClueSet:
    """Internal implementation of clue generation with structured output."""
    # Use structured output - LLM is constrained to output valid Pydantic
    llm = ChatOpenAI(
        model="gpt-4o",
        temperature=0.8,
        api_key=os.getenv("OPENAI_API_KEY"),
    ).with_structured_output(ClueSet)
    
    # Get all suspect roles for alibi verification
    all_suspect_roles = ", ".join(skeleton.suspect_briefs)
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are designing CLUES for a murder mystery game.

CASE DETAILS:
- Setting: {setting}
- Victim: {victim_name} - {victim_background}
- The Murderer's Role: {murderer_role}
- Weapon: {weapon}
- Motive: {motive}
- Murder Time: Around {murder_time}

ALL SUSPECT ROLES: {all_suspect_roles}

CLUE DESIGN RULES:
1. Create 5 clues spread across these locations: {locations}
2. 3-4 clues should POINT TOWARD the murderer when combined
3. 1 clue should be a RED HERRING pointing at an innocent suspect
4. Clues should be DISCOVERABLE THINGS: documents, objects, traces, marks

ALIBI VERIFICATION (CRITICAL):
Each clue should help verify or contradict alibis:
- contradicts_alibi_of_role: Role of suspect whose alibi this DISPROVES
  * At least ONE clue MUST contradict the murderer's alibi!
  * Example: Guest book shows murderer wasn't where they claimed
- supports_alibi_of_role: Role of suspect whose alibi this CONFIRMS
  * Some clues should SUPPORT innocent suspects' alibis
- timeline_implication: What this tells us about timing
  * Example: "Clock in photo shows 8:45 PM"
- evidence_type: 'physical' (forensic), 'documentary' (records/letters), or 'circumstantial'

REQUIRED CLUE DISTRIBUTION:
- 1 clue MUST contradict the murderer's false alibi (this is key evidence!)
- 1-2 clues should support innocent suspects' alibis
- 1-2 clues should point to motive/opportunity
- 1 red herring that initially looks damning but has innocent explanation

PUZZLE DESIGN:
- No single clue solves it alone - need to combine evidence
- The murderer's alibi can be proven FALSE through physical evidence
- Innocent suspects' alibis should be verifiable
- The solution should feel "obvious in hindsight" """),
        ("human", """Create 5 interconnected clues at these locations:
{locations}

Design them as a coherent puzzle pointing to "{murderer_role}".
At least ONE clue must DISPROVE the murderer's alibi.
Include support for innocent alibis and one red herring.""")
    ])
    
    chain = prompt | llm
    
    # Structured output returns the Pydantic model directly - no parsing needed!
    clue_set = await chain.ainvoke({
        "setting": skeleton.setting,
        "victim_name": skeleton.victim_name,
        "victim_background": skeleton.victim_background,
        "murderer_role": murderer_role,
        "weapon": skeleton.weapon,
        "motive": skeleton.motive,
        "murder_time": skeleton.murder_time,
        "all_suspect_roles": all_suspect_roles,
        "locations": ", ".join(skeleton.clue_locations),
    })
    
    return clue_set


async def generate_clues(
    skeleton: MysterySkeleton,
    murderer_role: str,
) -> ClueSet:
    """Generate all clues for the mystery.
    
    Uses gpt-4o for quality clue design that forms a coherent puzzle.
    Uses structured output for reliable parsing with automatic retries.
    
    Note: Takes murderer_role (not name) so this can run in PARALLEL
    with suspect generation. The role is enough to design clues.
    """
    logger.info("[PARALLEL] Generating clues for %d locations...", len(skeleton.clue_locations))
    
    clue_set = await retry_with_backoff(_generate_clues_impl, skeleton, murderer_role)
    
    logger.info("[PARALLEL] ✓ Generated %d clues", len(clue_set.clues))
    return clue_set


# =============================================================================
# ASSEMBLY: COMBINE SUB-AGENT OUTPUTS
# =============================================================================

def assemble_mystery(
    skeleton: MysterySkeleton,
    suspect_results: List[Tuple[SuspectDraft, int, bool]],
    clue_set: ClueSet,
    voice_summary: Optional[str] = None,
) -> Mystery:
    """Assemble final Mystery from sub-agent outputs.
    
    Combines all parallel outputs into the final Mystery model.
    Also handles voice assignment, location hints, and alibi verification wiring.
    """
    # Sort suspects by their original index to maintain order
    suspect_results.sort(key=lambda x: x[1])
    
    # Build a mapping from role brief -> suspect name for alibi resolution
    role_to_name = {}
    for draft, index, is_guilty in suspect_results:
        role_brief = skeleton.suspect_briefs[index].lower()
        role_to_name[role_brief] = draft.name
        # Also map partial matches
        for word in role_brief.split():
            if len(word) > 3:  # Skip short words like "the"
                role_to_name[word] = draft.name
    
    # Get unique locations from clues for assignment to suspects
    clue_locations = list(set(clue.location for clue in clue_set.clues))
    
    # Convert drafts to full Suspect models with alibi verification
    suspects: List[Suspect] = []
    murderer_name = None
    all_witness_statements: List[WitnessStatement] = []
    
    for idx, (draft, index, is_guilty) in enumerate(suspect_results):
        # Assign a location hint to each suspect
        location_hint = clue_locations[idx] if idx < len(clue_locations) else None
        
        # Convert AlibiDraft to AlibiClaim
        structured_alibi = None
        if hasattr(draft, 'structured_alibi') and draft.structured_alibi:
            alibi_draft = draft.structured_alibi
            # Resolve corroborator role to name
            corroborator_name = None
            if alibi_draft.corroborator:
                corroborator_lower = alibi_draft.corroborator.lower()
                for key, name in role_to_name.items():
                    if key in corroborator_lower or corroborator_lower in key:
                        corroborator_name = name
                        break
            
            structured_alibi = AlibiClaim(
                time_claimed=alibi_draft.time_claimed,
                location_claimed=alibi_draft.location_claimed,
                activity=alibi_draft.activity,
                corroborator=corroborator_name,
                corroboration_type=alibi_draft.corroboration_type,
                is_truthful=alibi_draft.is_truthful,
            )
        
        # Build witness statements from this suspect
        witness_statements = []
        if hasattr(draft, 'witness_claim') and draft.witness_claim and hasattr(draft, 'witness_subject_role') and draft.witness_subject_role:
            # Resolve the subject role to a name
            subject_name = None
            subject_lower = draft.witness_subject_role.lower()
            for key, name in role_to_name.items():
                if key in subject_lower or subject_lower in key:
                    subject_name = name
                    break
            
            if subject_name:
                ws = WitnessStatement(
                    witness=draft.name,
                    subject=subject_name,
                    claim=draft.witness_claim,
                    time_of_sighting="",  # Extracted from claim
                    location_of_sighting="",  # Extracted from claim
                    is_truthful=not is_guilty,  # Guilty suspect might lie
                )
                witness_statements.append(ws)
                all_witness_statements.append(ws)
        
        suspect = Suspect(
            name=draft.name,
            role=draft.role,
            personality=draft.personality,
            alibi=draft.alibi,
            secret=draft.secret,
            clue_they_know=draft.clue_they_know,
            isGuilty=is_guilty,
            gender=draft.gender,
            age=draft.age,
            nationality=draft.nationality,
            voice_id=None,  # Assigned below
            portrait_path=None,
            location_hint=location_hint,
            structured_alibi=structured_alibi,
            witness_statements=witness_statements,
        )
        suspects.append(suspect)
        
        if is_guilty:
            murderer_name = draft.name
    
    # Log alibi assignments
    for s in suspects:
        alibi_info = f"truthful={s.structured_alibi.is_truthful}" if s.structured_alibi else "none"
        logger.info("[PARALLEL] Suspect %s: location=%s, alibi=%s", 
                   s.name, s.location_hint, alibi_info)
    
    # Convert clue drafts to Clue models with alibi verification
    clues: List[Clue] = []
    for clue_draft in clue_set.clues:
        # Resolve role references to suspect names
        contradicts_name = None
        if hasattr(clue_draft, 'contradicts_alibi_of_role') and clue_draft.contradicts_alibi_of_role:
            role_lower = clue_draft.contradicts_alibi_of_role.lower()
            for key, name in role_to_name.items():
                if key in role_lower or role_lower in key:
                    contradicts_name = name
                    break
        
        supports_name = None
        if hasattr(clue_draft, 'supports_alibi_of_role') and clue_draft.supports_alibi_of_role:
            role_lower = clue_draft.supports_alibi_of_role.lower()
            for key, name in role_to_name.items():
                if key in role_lower or role_lower in key:
                    supports_name = name
                    break
        
        clue = Clue(
            id=clue_draft.id,
            description=clue_draft.description,
            location=clue_draft.location,
            significance=clue_draft.significance,
            contradicts_alibi_of=contradicts_name,
            supports_alibi_of=supports_name,
            timeline_implication=getattr(clue_draft, 'timeline_implication', None),
            evidence_type=getattr(clue_draft, 'evidence_type', 'circumstantial'),
        )
        clues.append(clue)
    
    # Log clue alibi verification
    for c in clues:
        if c.contradicts_alibi_of or c.supports_alibi_of:
            logger.info("[PARALLEL] Clue %s: contradicts=%s, supports=%s",
                       c.id, c.contradicts_alibi_of, c.supports_alibi_of)
    
    # Assign voices if available
    if voice_summary:
        suspects = _assign_voices_to_suspects(suspects, voice_summary)
    
    # Build murder method with evidence trail
    evidence_trail = [c.id for c in clues if c.contradicts_alibi_of == murderer_name]
    murder_method = MurderMethod(
        weapon=skeleton.weapon,
        time_of_death=skeleton.murder_time,
        location_of_murder=skeleton.murder_location or skeleton.clue_locations[0],
        opportunity=f"While other suspects were occupied, during the time window around {skeleton.murder_time}",
        evidence_trail=evidence_trail,
    )
    
    mystery = Mystery(
        setting=skeleton.setting,
        victim=Victim(
            name=skeleton.victim_name,
            background=skeleton.victim_background,
        ),
        murderer=murderer_name or "Unknown",
        weapon=skeleton.weapon,
        motive=skeleton.motive,
        suspects=suspects,
        clues=clues,
        murder_method=murder_method,
        witness_statements=all_witness_statements,
    )
    
    logger.info(
        "[PARALLEL] Assembled mystery: %s murdered by %s with %s",
        skeleton.victim_name,
        murderer_name,
        skeleton.weapon,
    )
    logger.info(
        "[PARALLEL] Alibi verification: %d witness statements, %d clues with alibi implications",
        len(all_witness_statements),
        len([c for c in clues if c.contradicts_alibi_of or c.supports_alibi_of])
    )
    return mystery


def _assign_voices_to_suspects(
    suspects: List[Suspect],
    voice_summary: str,
) -> List[Suspect]:
    """Assign voices to suspects based on characteristics."""
    try:
        from services.voice_service import get_voice_service
        
        voice_service = get_voice_service()
        if not voice_service.is_available:
            logger.info("[PARALLEL] Voice service not available, skipping voice assignment")
            return suspects
        
        available_voices = voice_service.get_available_voices()
        if not available_voices:
            return suspects
        
        used_ids = set()
        
        for suspect in suspects:
            suspect_dict = {
                "name": suspect.name,
                "role": suspect.role,
                "personality": suspect.personality,
                "gender": suspect.gender,
                "age": suspect.age,
                "nationality": suspect.nationality,
            }
            
            voice = voice_service.match_voice_to_suspect(
                suspect_dict,
                available_voices,
                list(used_ids),
            )
            
            if voice:
                suspect.voice_id = voice.voice_id
                used_ids.add(voice.voice_id)
                logger.info(
                    "[PARALLEL] Assigned voice '%s' to %s",
                    voice.name,
                    suspect.name,
                )
        
        return suspects
        
    except Exception as e:
        logger.warning("[PARALLEL] Voice assignment failed: %s", e)
        return suspects


# =============================================================================
# MAIN PARALLEL GENERATION FUNCTION
# =============================================================================

async def generate_mystery_parallel(
    premise: Optional[MysteryPremise] = None,
    config: Optional[MysteryConfig] = None,
    voice_summary: Optional[str] = None,
) -> Mystery:
    """Generate a complete mystery using parallel sub-agents.
    
    Performance: ~8-10s vs ~15s for monolithic generation.
    
    This is NOT the agent that users talk to. Users interact with the
    Game Master Agent in services/agent.py. This function only runs once
    at game start to generate the mystery content.
    
    Architecture:
        1. Skeleton Agent (gpt-4o-mini, ~6s) - Framework
        2. PARALLEL: 4x Suspects + Clues (~5s total, not sequential!)
        3. Assembly + Voice Assignment
    
    Key optimization: Suspects and clues run IN PARALLEL because clues
    only need the murderer's ROLE (from skeleton), not their NAME.
    
    Args:
        premise: Optional preset premise (setting, victim)
        config: Game configuration for difficulty/tone
        voice_summary: Available voices for assignment
        
    Returns:
        Complete Mystery object ready for gameplay
    """
    import time
    t_start = time.perf_counter()
    
    # =========================================================================
    # STAGE 1: SKELETON (~6s)
    # =========================================================================
    logger.info("[PARALLEL] ═══ Stage 1: Generating skeleton ═══")
    t1 = time.perf_counter()
    
    skeleton = await generate_skeleton(config=config, premise=premise)
    
    t2 = time.perf_counter()
    logger.info("[PARALLEL] Skeleton complete in %.2fs", t2 - t1)
    
    # =========================================================================
    # STAGE 2: PARALLEL SUSPECTS + CLUES (~5s total, not 8s sequential!)
    # =========================================================================
    logger.info("[PARALLEL] ═══ Stage 2: Generating suspects + clues IN PARALLEL ═══")
    t3 = time.perf_counter()
    
    # Get murderer role for clue generation (don't need name yet!)
    murderer_role = skeleton.suspect_briefs[skeleton.murderer_index]
    
    # Create tasks for each suspect
    suspect_tasks = []
    for i, role_brief in enumerate(skeleton.suspect_briefs):
        is_guilty = (i == skeleton.murderer_index)
        task = generate_suspect(
            skeleton=skeleton,
            role_brief=role_brief,
            suspect_index=i,
            is_guilty=is_guilty,
            voice_options=voice_summary[:2000] if voice_summary else None,
        )
        suspect_tasks.append(task)
    
    # Create clue task - runs IN PARALLEL with suspects!
    clue_task = generate_clues(skeleton, murderer_role)
    
    # Run ALL tasks in parallel: 4 suspects + 1 clue generation
    logger.info("[PARALLEL] Launching %d suspect tasks + 1 clue task (all parallel)", 
                len(suspect_tasks))
    all_tasks = suspect_tasks + [clue_task]
    all_results = await asyncio.gather(*all_tasks, return_exceptions=True)
    
    # Split results: first 4 are suspects, last is clues
    suspect_results = all_results[:4]
    clue_result = all_results[4]
    
    # Handle suspect failures
    valid_suspect_results = []
    for i, result in enumerate(suspect_results):
        if isinstance(result, Exception):
            logger.error("[PARALLEL] Suspect %d failed: %s", i, result)
            raise result
        valid_suspect_results.append(result)
    
    # Handle clue failure
    if isinstance(clue_result, Exception):
        logger.error("[PARALLEL] Clue generation failed: %s", clue_result)
        raise clue_result
    clue_set = clue_result
    
    t4 = time.perf_counter()
    logger.info("[PARALLEL] All %d suspects + clues complete in %.2fs (PARALLEL)", 
                len(valid_suspect_results), t4 - t3)
    
    # =========================================================================
    # STAGE 3: ASSEMBLY + VOICE ASSIGNMENT
    # =========================================================================
    logger.info("[PARALLEL] ═══ Stage 3: Assembly ═══")
    
    mystery = assemble_mystery(
        skeleton=skeleton,
        suspect_results=valid_suspect_results,
        clue_set=clue_set,
        voice_summary=voice_summary,
    )
    
    t_end = time.perf_counter()
    
    # Log voice assignment stats
    assigned_count = sum(1 for s in mystery.suspects if s.voice_id)
    logger.info(
        "[PARALLEL] ✅ Mystery generation complete in %.2fs (voices: %d/%d)",
        t_end - t_start,
        assigned_count,
        len(mystery.suspects),
    )
    
    return mystery


# =============================================================================
# SYNC WRAPPER FOR EXISTING CODE
# =============================================================================

def generate_mystery_parallel_sync(
    premise: Optional[MysteryPremise] = None,
    config: Optional[MysteryConfig] = None,
    voice_summary: Optional[str] = None,
) -> Mystery:
    """Synchronous wrapper for parallel mystery generation.
    
    Drop-in replacement for generate_mystery() in existing code.
    Handles the async/sync boundary.
    """
    try:
        # Try to get existing event loop
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # We're inside an async context, need to use nest_asyncio or thread
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(
                    asyncio.run,
                    generate_mystery_parallel(premise, config, voice_summary)
                )
                return future.result()
        else:
            return loop.run_until_complete(
                generate_mystery_parallel(premise, config, voice_summary)
            )
    except RuntimeError:
        # No event loop exists, create one
        return asyncio.run(
            generate_mystery_parallel(premise, config, voice_summary)
        )

